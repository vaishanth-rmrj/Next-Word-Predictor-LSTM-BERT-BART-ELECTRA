{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyMyBBbiNQ38E0rZPn+SbG6N"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["Mounting google drive"],"metadata":{"id":"g2CAMyS2wCIW"}},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount(\"/content/drive/\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6tzSmIyWwGfZ","executionInfo":{"status":"ok","timestamp":1674748155772,"user_tz":300,"elapsed":876,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"05490cc0-c2d5-44c5-b80a-da6b7e5e6703"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive/; to attempt to forcibly remount, call drive.mount(\"/content/drive/\", force_remount=True).\n"]}]},{"cell_type":"code","source":["# change to working directory\n","!cd /content/drive/MyDrive/self_projects/next_word_predictor"],"metadata":{"id":"rNEItw0XwS4r","executionInfo":{"status":"ok","timestamp":1674748276652,"user_tz":300,"elapsed":273,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":["### Importing libs"],"metadata":{"id":"bayiOdYOx0pO"}},{"cell_type":"code","source":["import tensorflow as tf\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.layers import Embedding, LSTM, Dense\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras.optimizers import Adam\n","import pickle\n","import numpy as np\n","import os"],"metadata":{"id":"UAmMuU3xxsT4","executionInfo":{"status":"ok","timestamp":1674748345007,"user_tz":300,"elapsed":3849,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":["### Loading and processing dataset"],"metadata":{"id":"lqQq45o1yAlQ"}},{"cell_type":"code","source":["with open(\"assets/metamorphosis_clean.txt\", 'r', encoding = \"utf8\") as f:\n","  txt = f.read()\n","  "],"metadata":{"id":"a1_fvp6ox6qW","executionInfo":{"status":"ok","timestamp":1674749100458,"user_tz":300,"elapsed":854,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":26,"outputs":[]},{"cell_type":"code","source":["# data cleaning\n","txt = txt.replace('\\n', '').replace('\\r', '').replace('\\ufeff', '').replace('\\'', '')\n","txt_seq = txt.split('.')"],"metadata":{"id":"7cPV2Ri_yb1u","executionInfo":{"status":"ok","timestamp":1674749100460,"user_tz":300,"elapsed":33,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":27,"outputs":[]},{"cell_type":"markdown","source":["#### Tokenization"],"metadata":{"id":"THp6F_uT1FhJ"}},{"cell_type":"code","source":["# initializing the tokenizer\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(txt_seq)\n","print(\"Vocab size: \", len(tokenizer.word_index) + 1)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"2gW6vGzDypGX","executionInfo":{"status":"ok","timestamp":1674749510238,"user_tz":300,"elapsed":600,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"5966cff4-7593-4fa6-8aac-9af7462cb028"},"execution_count":35,"outputs":[{"output_type":"stream","name":"stdout","text":["Vocab size:  3749\n"]}]},{"cell_type":"code","source":["# saving the tokenizer for predict function.\n","pickle.dump(tokenizer, open('tokenizer.pkl', 'wb'))"],"metadata":{"id":"zDJnHZUWzYFl","executionInfo":{"status":"ok","timestamp":1674749517745,"user_tz":300,"elapsed":128,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":36,"outputs":[]},{"cell_type":"code","source":["encoded_txt = tokenizer.texts_to_sequences(txt_seq)\n","print(\"Original text: \", txt_seq[0])\n","print(\"Encoded text: \", encoded_txt[0])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"5YDQ-liB1wXp","executionInfo":{"status":"ok","timestamp":1674750144179,"user_tz":300,"elapsed":9,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"d3ca70dd-43e4-4af3-de70-92d2e3fafb63"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["Original text:  One morning, when Gregor Samsa woke from troubled dreams, he foundhimself transformed in his bed into a horrible vermin\n","Encoded text:  [51, 146, 55, 14, 83, 846, 31, 1287, 847, 4, 1288, 1289, 9, 5, 105, 44, 12, 652, 1290]\n"]}]},{"cell_type":"code","source":["# creating next word sequnces\n","seq = []\n","\n","for txt in encoded_txt:\n","  for i in range(len(txt)):\n","    if i > 0 and i < len(txt):\n","      seq.append(txt[i-1:i+1])\n","\n","print(\"Encoded text pairs: \", seq[:len(encoded_txt[0])-1])\n","print(\"Total encoded text pairs:\", len(seq))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_kYJFKD2FAE","executionInfo":{"status":"ok","timestamp":1674750366328,"user_tz":300,"elapsed":11,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"63206cb7-549a-412d-d962-c648078328a3"},"execution_count":49,"outputs":[{"output_type":"stream","name":"stdout","text":["Encoded text pairs:  [[51, 146], [146, 55], [55, 14], [14, 83], [83, 846], [846, 31], [31, 1287], [1287, 847], [847, 4], [4, 1288], [1288, 1289], [1289, 9], [9, 5], [5, 105], [105, 44], [44, 12], [12, 652], [652, 1290]]\n","Total encoded text pairs: 19735\n"]}]},{"cell_type":"code","source":["# x and y dataset\n","x = np.array(seq)[:, 0]\n","y = np.array(seq)[:, 1]\n","# converting to one hot encoding\n","y = to_categorical(y, num_classes=len(tokenizer.word_index) + 1)"],"metadata":{"id":"2VA3H8YD4U8B","executionInfo":{"status":"ok","timestamp":1674750584976,"user_tz":300,"elapsed":137,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":58,"outputs":[]},{"cell_type":"markdown","source":["#### Next Word Predictor Model"],"metadata":{"id":"PN8MwoZ86tL3"}},{"cell_type":"code","source":["vocab_size = len(tokenizer.word_index) + 1\n","\n","model = Sequential()\n","model.add(Embedding(vocab_size, 10, input_length=1))\n","model.add(LSTM(1000, return_sequences=True))\n","model.add(LSTM(1000))\n","model.add(Dense(1000, activation=\"relu\"))\n","model.add(Dense(vocab_size, activation=\"softmax\"))\n","\n","model.summary()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uAjOjRYg53cj","executionInfo":{"status":"ok","timestamp":1674750771497,"user_tz":300,"elapsed":3565,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"70e70521-08aa-4841-aff4-a5dd55d865b9"},"execution_count":63,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," embedding (Embedding)       (None, 1, 10)             37490     \n","                                                                 \n"," lstm (LSTM)                 (None, 1, 1000)           4044000   \n","                                                                 \n"," lstm_1 (LSTM)               (None, 1000)              8004000   \n","                                                                 \n"," dense (Dense)               (None, 1000)              1001000   \n","                                                                 \n"," dense_1 (Dense)             (None, 3749)              3752749   \n","                                                                 \n","=================================================================\n","Total params: 16,839,239\n","Trainable params: 16,839,239\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}]},{"cell_type":"code","source":["# visualizing the model\n","from tensorflow import keras\n","from keras.utils.vis_utils import plot_model\n","\n","keras.utils.plot_model(model, to_file='model.png', show_layer_names=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":564},"id":"T3254bZ67LG-","executionInfo":{"status":"ok","timestamp":1674750810757,"user_tz":300,"elapsed":466,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"16c5ba3b-522e-408e-b268-e1cacd351b83"},"execution_count":64,"outputs":[{"output_type":"execute_result","data":{"image/png":"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\n","text/plain":["<IPython.core.display.Image object>"]},"metadata":{},"execution_count":64}]},{"cell_type":"code","source":["from tensorflow.keras.callbacks import ModelCheckpoint\n","from tensorflow.keras.callbacks import ReduceLROnPlateau\n","from tensorflow.keras.callbacks import TensorBoard\n","\n","checkpoint = ModelCheckpoint(\"nextword1.h5\", monitor='loss', verbose=1,\n","    save_best_only=True, mode='auto')\n","\n","reduce = ReduceLROnPlateau(monitor='loss', factor=0.2, patience=3, min_lr=0.0001, verbose = 1)\n","\n","logdir='logsnextword1'\n","tensorboard_Visualization = TensorBoard(log_dir=logdir)"],"metadata":{"id":"czzVVKyS7VfB","executionInfo":{"status":"ok","timestamp":1674750841281,"user_tz":300,"elapsed":127,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["model.compile(loss=\"categorical_crossentropy\", optimizer=Adam(lr=0.001))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"NcsUcJlu7c7C","executionInfo":{"status":"ok","timestamp":1674750866806,"user_tz":300,"elapsed":12,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"dccf91b4-6938-4543-e66b-dbe4efa47c92"},"execution_count":66,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/keras/optimizers/optimizer_v2/adam.py:110: UserWarning: The `lr` argument is deprecated, use `learning_rate` instead.\n","  super(Adam, self).__init__(name, **kwargs)\n"]}]},{"cell_type":"code","source":["model.fit(x, y, epochs=150, batch_size=64, callbacks=[checkpoint, reduce, tensorboard_Visualization])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9HO1ZGK_7jFi","executionInfo":{"status":"ok","timestamp":1674761686909,"user_tz":300,"elapsed":10702191,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"f8d98d73-8199-4cb2-b3ce-f5b6302f13da"},"execution_count":69,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/150\n","309/309 [==============================] - ETA: 0s - loss: 6.3580\n","Epoch 1: loss improved from inf to 6.35800, saving model to nextword1.h5\n","309/309 [==============================] - 68s 219ms/step - loss: 6.3580 - lr: 0.0010\n","Epoch 2/150\n","309/309 [==============================] - ETA: 0s - loss: 6.1257\n","Epoch 2: loss improved from 6.35800 to 6.12570, saving model to nextword1.h5\n","309/309 [==============================] - 70s 226ms/step - loss: 6.1257 - lr: 0.0010\n","Epoch 3/150\n","309/309 [==============================] - ETA: 0s - loss: 5.8770\n","Epoch 3: loss improved from 6.12570 to 5.87699, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 5.8770 - lr: 0.0010\n","Epoch 4/150\n","309/309 [==============================] - ETA: 0s - loss: 5.6394\n","Epoch 4: loss improved from 5.87699 to 5.63936, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 5.6394 - lr: 0.0010\n","Epoch 5/150\n","309/309 [==============================] - ETA: 0s - loss: 5.4154\n","Epoch 5: loss improved from 5.63936 to 5.41538, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 5.4154 - lr: 0.0010\n","Epoch 6/150\n","309/309 [==============================] - ETA: 0s - loss: 5.2615\n","Epoch 6: loss improved from 5.41538 to 5.26152, saving model to nextword1.h5\n","309/309 [==============================] - 71s 228ms/step - loss: 5.2615 - lr: 0.0010\n","Epoch 7/150\n","309/309 [==============================] - ETA: 0s - loss: 5.1360\n","Epoch 7: loss improved from 5.26152 to 5.13596, saving model to nextword1.h5\n","309/309 [==============================] - 73s 236ms/step - loss: 5.1360 - lr: 0.0010\n","Epoch 8/150\n","309/309 [==============================] - ETA: 0s - loss: 5.0084\n","Epoch 8: loss improved from 5.13596 to 5.00837, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 5.0084 - lr: 0.0010\n","Epoch 9/150\n","309/309 [==============================] - ETA: 0s - loss: 4.8851\n","Epoch 9: loss improved from 5.00837 to 4.88515, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 4.8851 - lr: 0.0010\n","Epoch 10/150\n","309/309 [==============================] - ETA: 0s - loss: 4.7730\n","Epoch 10: loss improved from 4.88515 to 4.77299, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 4.7730 - lr: 0.0010\n","Epoch 11/150\n","309/309 [==============================] - ETA: 0s - loss: 4.6669\n","Epoch 11: loss improved from 4.77299 to 4.66687, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 4.6669 - lr: 0.0010\n","Epoch 12/150\n","309/309 [==============================] - ETA: 0s - loss: 4.5824\n","Epoch 12: loss improved from 4.66687 to 4.58241, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 4.5824 - lr: 0.0010\n","Epoch 13/150\n","309/309 [==============================] - ETA: 0s - loss: 4.4975\n","Epoch 13: loss improved from 4.58241 to 4.49755, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 4.4975 - lr: 0.0010\n","Epoch 14/150\n","309/309 [==============================] - ETA: 0s - loss: 4.4145\n","Epoch 14: loss improved from 4.49755 to 4.41449, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 4.4145 - lr: 0.0010\n","Epoch 15/150\n","309/309 [==============================] - ETA: 0s - loss: 4.3368\n","Epoch 15: loss improved from 4.41449 to 4.33680, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 4.3368 - lr: 0.0010\n","Epoch 16/150\n","309/309 [==============================] - ETA: 0s - loss: 4.2578\n","Epoch 16: loss improved from 4.33680 to 4.25782, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 4.2578 - lr: 0.0010\n","Epoch 17/150\n","309/309 [==============================] - ETA: 0s - loss: 4.1810\n","Epoch 17: loss improved from 4.25782 to 4.18101, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 4.1810 - lr: 0.0010\n","Epoch 18/150\n","309/309 [==============================] - ETA: 0s - loss: 4.1096\n","Epoch 18: loss improved from 4.18101 to 4.10958, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 4.1096 - lr: 0.0010\n","Epoch 19/150\n","309/309 [==============================] - ETA: 0s - loss: 4.0385\n","Epoch 19: loss improved from 4.10958 to 4.03847, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 4.0385 - lr: 0.0010\n","Epoch 20/150\n","309/309 [==============================] - ETA: 0s - loss: 3.9730\n","Epoch 20: loss improved from 4.03847 to 3.97297, saving model to nextword1.h5\n","309/309 [==============================] - 71s 228ms/step - loss: 3.9730 - lr: 0.0010\n","Epoch 21/150\n","309/309 [==============================] - ETA: 0s - loss: 3.9138\n","Epoch 21: loss improved from 3.97297 to 3.91375, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 3.9138 - lr: 0.0010\n","Epoch 22/150\n","309/309 [==============================] - ETA: 0s - loss: 3.8618\n","Epoch 22: loss improved from 3.91375 to 3.86181, saving model to nextword1.h5\n","309/309 [==============================] - 71s 228ms/step - loss: 3.8618 - lr: 0.0010\n","Epoch 23/150\n","309/309 [==============================] - ETA: 0s - loss: 3.8091\n","Epoch 23: loss improved from 3.86181 to 3.80908, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.8091 - lr: 0.0010\n","Epoch 24/150\n","309/309 [==============================] - ETA: 0s - loss: 3.7579\n","Epoch 24: loss improved from 3.80908 to 3.75791, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.7579 - lr: 0.0010\n","Epoch 25/150\n","309/309 [==============================] - ETA: 0s - loss: 3.7144\n","Epoch 25: loss improved from 3.75791 to 3.71443, saving model to nextword1.h5\n","309/309 [==============================] - 73s 236ms/step - loss: 3.7144 - lr: 0.0010\n","Epoch 26/150\n","309/309 [==============================] - ETA: 0s - loss: 3.6699\n","Epoch 26: loss improved from 3.71443 to 3.66988, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.6699 - lr: 0.0010\n","Epoch 27/150\n","309/309 [==============================] - ETA: 0s - loss: 3.6316\n","Epoch 27: loss improved from 3.66988 to 3.63160, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.6316 - lr: 0.0010\n","Epoch 28/150\n","309/309 [==============================] - ETA: 0s - loss: 3.5976\n","Epoch 28: loss improved from 3.63160 to 3.59756, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.5976 - lr: 0.0010\n","Epoch 29/150\n","309/309 [==============================] - ETA: 0s - loss: 3.5630\n","Epoch 29: loss improved from 3.59756 to 3.56299, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 3.5630 - lr: 0.0010\n","Epoch 30/150\n","309/309 [==============================] - ETA: 0s - loss: 3.5355\n","Epoch 30: loss improved from 3.56299 to 3.53547, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.5355 - lr: 0.0010\n","Epoch 31/150\n","309/309 [==============================] - ETA: 0s - loss: 3.5076\n","Epoch 31: loss improved from 3.53547 to 3.50757, saving model to nextword1.h5\n","309/309 [==============================] - 73s 237ms/step - loss: 3.5076 - lr: 0.0010\n","Epoch 32/150\n","309/309 [==============================] - ETA: 0s - loss: 3.4790\n","Epoch 32: loss improved from 3.50757 to 3.47901, saving model to nextword1.h5\n","309/309 [==============================] - 71s 228ms/step - loss: 3.4790 - lr: 0.0010\n","Epoch 33/150\n","309/309 [==============================] - ETA: 0s - loss: 3.4561\n","Epoch 33: loss improved from 3.47901 to 3.45610, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 3.4561 - lr: 0.0010\n","Epoch 34/150\n","309/309 [==============================] - ETA: 0s - loss: 3.4301\n","Epoch 34: loss improved from 3.45610 to 3.43009, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.4301 - lr: 0.0010\n","Epoch 35/150\n","309/309 [==============================] - ETA: 0s - loss: 3.4075\n","Epoch 35: loss improved from 3.43009 to 3.40750, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.4075 - lr: 0.0010\n","Epoch 36/150\n","309/309 [==============================] - ETA: 0s - loss: 3.3917\n","Epoch 36: loss improved from 3.40750 to 3.39168, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.3917 - lr: 0.0010\n","Epoch 37/150\n","309/309 [==============================] - ETA: 0s - loss: 3.3716\n","Epoch 37: loss improved from 3.39168 to 3.37159, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.3716 - lr: 0.0010\n","Epoch 38/150\n","309/309 [==============================] - ETA: 0s - loss: 3.3536\n","Epoch 38: loss improved from 3.37159 to 3.35359, saving model to nextword1.h5\n","309/309 [==============================] - 72s 231ms/step - loss: 3.3536 - lr: 0.0010\n","Epoch 39/150\n","309/309 [==============================] - ETA: 0s - loss: 3.3395\n","Epoch 39: loss improved from 3.35359 to 3.33946, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.3395 - lr: 0.0010\n","Epoch 40/150\n","309/309 [==============================] - ETA: 0s - loss: 3.3269\n","Epoch 40: loss improved from 3.33946 to 3.32688, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.3269 - lr: 0.0010\n","Epoch 41/150\n","309/309 [==============================] - ETA: 0s - loss: 3.3141\n","Epoch 41: loss improved from 3.32688 to 3.31408, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 3.3141 - lr: 0.0010\n","Epoch 42/150\n","309/309 [==============================] - ETA: 0s - loss: 3.3011\n","Epoch 42: loss improved from 3.31408 to 3.30112, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.3011 - lr: 0.0010\n","Epoch 43/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2875\n","Epoch 43: loss improved from 3.30112 to 3.28747, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.2875 - lr: 0.0010\n","Epoch 44/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2760\n","Epoch 44: loss improved from 3.28747 to 3.27601, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 3.2760 - lr: 0.0010\n","Epoch 45/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2674\n","Epoch 45: loss improved from 3.27601 to 3.26741, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.2674 - lr: 0.0010\n","Epoch 46/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2556\n","Epoch 46: loss improved from 3.26741 to 3.25557, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.2556 - lr: 0.0010\n","Epoch 47/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2434\n","Epoch 47: loss improved from 3.25557 to 3.24336, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 3.2434 - lr: 0.0010\n","Epoch 48/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2377\n","Epoch 48: loss improved from 3.24336 to 3.23771, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 3.2377 - lr: 0.0010\n","Epoch 49/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2304\n","Epoch 49: loss improved from 3.23771 to 3.23039, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.2304 - lr: 0.0010\n","Epoch 50/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2210\n","Epoch 50: loss improved from 3.23039 to 3.22103, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.2210 - lr: 0.0010\n","Epoch 51/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2121\n","Epoch 51: loss improved from 3.22103 to 3.21211, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.2121 - lr: 0.0010\n","Epoch 52/150\n","309/309 [==============================] - ETA: 0s - loss: 3.2060\n","Epoch 52: loss improved from 3.21211 to 3.20605, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 3.2060 - lr: 0.0010\n","Epoch 53/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1974\n","Epoch 53: loss improved from 3.20605 to 3.19739, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.1974 - lr: 0.0010\n","Epoch 54/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1873\n","Epoch 54: loss improved from 3.19739 to 3.18730, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 3.1873 - lr: 0.0010\n","Epoch 55/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1843\n","Epoch 55: loss improved from 3.18730 to 3.18429, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.1843 - lr: 0.0010\n","Epoch 56/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1755\n","Epoch 56: loss improved from 3.18429 to 3.17546, saving model to nextword1.h5\n","309/309 [==============================] - 72s 232ms/step - loss: 3.1755 - lr: 0.0010\n","Epoch 57/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1721\n","Epoch 57: loss improved from 3.17546 to 3.17209, saving model to nextword1.h5\n","309/309 [==============================] - 70s 226ms/step - loss: 3.1721 - lr: 0.0010\n","Epoch 58/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1648\n","Epoch 58: loss improved from 3.17209 to 3.16479, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.1648 - lr: 0.0010\n","Epoch 59/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1632\n","Epoch 59: loss improved from 3.16479 to 3.16317, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.1632 - lr: 0.0010\n","Epoch 60/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1568\n","Epoch 60: loss improved from 3.16317 to 3.15684, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.1568 - lr: 0.0010\n","Epoch 61/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1502\n","Epoch 61: loss improved from 3.15684 to 3.15024, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.1502 - lr: 0.0010\n","Epoch 62/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1400\n","Epoch 62: loss improved from 3.15024 to 3.14004, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 3.1400 - lr: 0.0010\n","Epoch 63/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1360\n","Epoch 63: loss improved from 3.14004 to 3.13604, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.1360 - lr: 0.0010\n","Epoch 64/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1319\n","Epoch 64: loss improved from 3.13604 to 3.13185, saving model to nextword1.h5\n","309/309 [==============================] - 70s 226ms/step - loss: 3.1319 - lr: 0.0010\n","Epoch 65/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1262\n","Epoch 65: loss improved from 3.13185 to 3.12619, saving model to nextword1.h5\n","309/309 [==============================] - 72s 231ms/step - loss: 3.1262 - lr: 0.0010\n","Epoch 66/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1187\n","Epoch 66: loss improved from 3.12619 to 3.11872, saving model to nextword1.h5\n","309/309 [==============================] - 72s 231ms/step - loss: 3.1187 - lr: 0.0010\n","Epoch 67/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1164\n","Epoch 67: loss improved from 3.11872 to 3.11643, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.1164 - lr: 0.0010\n","Epoch 68/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1128\n","Epoch 68: loss improved from 3.11643 to 3.11284, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.1128 - lr: 0.0010\n","Epoch 69/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1106\n","Epoch 69: loss improved from 3.11284 to 3.11056, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.1106 - lr: 0.0010\n","Epoch 70/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1052\n","Epoch 70: loss improved from 3.11056 to 3.10521, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.1052 - lr: 0.0010\n","Epoch 71/150\n","309/309 [==============================] - ETA: 0s - loss: 3.1026\n","Epoch 71: loss improved from 3.10521 to 3.10255, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.1026 - lr: 0.0010\n","Epoch 72/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0966\n","Epoch 72: loss improved from 3.10255 to 3.09657, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.0966 - lr: 0.0010\n","Epoch 73/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0940\n","Epoch 73: loss improved from 3.09657 to 3.09400, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 3.0940 - lr: 0.0010\n","Epoch 74/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0905\n","Epoch 74: loss improved from 3.09400 to 3.09047, saving model to nextword1.h5\n","309/309 [==============================] - 72s 232ms/step - loss: 3.0905 - lr: 0.0010\n","Epoch 75/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0871\n","Epoch 75: loss improved from 3.09047 to 3.08714, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.0871 - lr: 0.0010\n","Epoch 76/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0846\n","Epoch 76: loss improved from 3.08714 to 3.08465, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.0846 - lr: 0.0010\n","Epoch 77/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0805\n","Epoch 77: loss improved from 3.08465 to 3.08047, saving model to nextword1.h5\n","309/309 [==============================] - 71s 231ms/step - loss: 3.0805 - lr: 0.0010\n","Epoch 78/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0766\n","Epoch 78: loss improved from 3.08047 to 3.07665, saving model to nextword1.h5\n","309/309 [==============================] - 71s 231ms/step - loss: 3.0766 - lr: 0.0010\n","Epoch 79/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0714\n","Epoch 79: loss improved from 3.07665 to 3.07138, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 3.0714 - lr: 0.0010\n","Epoch 80/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0681\n","Epoch 80: loss improved from 3.07138 to 3.06814, saving model to nextword1.h5\n","309/309 [==============================] - 71s 228ms/step - loss: 3.0681 - lr: 0.0010\n","Epoch 81/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0651\n","Epoch 81: loss improved from 3.06814 to 3.06509, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.0651 - lr: 0.0010\n","Epoch 82/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0631\n","Epoch 82: loss improved from 3.06509 to 3.06309, saving model to nextword1.h5\n","309/309 [==============================] - 73s 237ms/step - loss: 3.0631 - lr: 0.0010\n","Epoch 83/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0590\n","Epoch 83: loss improved from 3.06309 to 3.05904, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.0590 - lr: 0.0010\n","Epoch 84/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0604\n","Epoch 84: loss did not improve from 3.05904\n","309/309 [==============================] - 69s 223ms/step - loss: 3.0604 - lr: 0.0010\n","Epoch 85/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0523\n","Epoch 85: loss improved from 3.05904 to 3.05225, saving model to nextword1.h5\n","309/309 [==============================] - 72s 232ms/step - loss: 3.0523 - lr: 0.0010\n","Epoch 86/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0528\n","Epoch 86: loss did not improve from 3.05225\n","309/309 [==============================] - 69s 222ms/step - loss: 3.0528 - lr: 0.0010\n","Epoch 87/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0518\n","Epoch 87: loss improved from 3.05225 to 3.05178, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.0518 - lr: 0.0010\n","Epoch 88/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0481\n","Epoch 88: loss improved from 3.05178 to 3.04806, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 3.0481 - lr: 0.0010\n","Epoch 89/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0462\n","Epoch 89: loss improved from 3.04806 to 3.04616, saving model to nextword1.h5\n","309/309 [==============================] - 71s 231ms/step - loss: 3.0462 - lr: 0.0010\n","Epoch 90/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0487\n","Epoch 90: loss did not improve from 3.04616\n","309/309 [==============================] - 69s 222ms/step - loss: 3.0487 - lr: 0.0010\n","Epoch 91/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0408\n","Epoch 91: loss improved from 3.04616 to 3.04084, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.0408 - lr: 0.0010\n","Epoch 92/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0382\n","Epoch 92: loss improved from 3.04084 to 3.03821, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.0382 - lr: 0.0010\n","Epoch 93/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0347\n","Epoch 93: loss improved from 3.03821 to 3.03470, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.0347 - lr: 0.0010\n","Epoch 94/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0355\n","Epoch 94: loss did not improve from 3.03470\n","309/309 [==============================] - 69s 222ms/step - loss: 3.0355 - lr: 0.0010\n","Epoch 95/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0316\n","Epoch 95: loss improved from 3.03470 to 3.03162, saving model to nextword1.h5\n","309/309 [==============================] - 70s 226ms/step - loss: 3.0316 - lr: 0.0010\n","Epoch 96/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0282\n","Epoch 96: loss improved from 3.03162 to 3.02815, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 3.0282 - lr: 0.0010\n","Epoch 97/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0258\n","Epoch 97: loss improved from 3.02815 to 3.02578, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.0258 - lr: 0.0010\n","Epoch 98/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0250\n","Epoch 98: loss improved from 3.02578 to 3.02499, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.0250 - lr: 0.0010\n","Epoch 99/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0233\n","Epoch 99: loss improved from 3.02499 to 3.02333, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.0233 - lr: 0.0010\n","Epoch 100/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0212\n","Epoch 100: loss improved from 3.02333 to 3.02120, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 3.0212 - lr: 0.0010\n","Epoch 101/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0191\n","Epoch 101: loss improved from 3.02120 to 3.01912, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 3.0191 - lr: 0.0010\n","Epoch 102/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0153\n","Epoch 102: loss improved from 3.01912 to 3.01527, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.0153 - lr: 0.0010\n","Epoch 103/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0151\n","Epoch 103: loss improved from 3.01527 to 3.01511, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 3.0151 - lr: 0.0010\n","Epoch 104/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0127\n","Epoch 104: loss improved from 3.01511 to 3.01273, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 3.0127 - lr: 0.0010\n","Epoch 105/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0107\n","Epoch 105: loss improved from 3.01273 to 3.01068, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 3.0107 - lr: 0.0010\n","Epoch 106/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0092\n","Epoch 106: loss improved from 3.01068 to 3.00921, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.0092 - lr: 0.0010\n","Epoch 107/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0073\n","Epoch 107: loss improved from 3.00921 to 3.00734, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 3.0073 - lr: 0.0010\n","Epoch 108/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0037\n","Epoch 108: loss improved from 3.00734 to 3.00367, saving model to nextword1.h5\n","309/309 [==============================] - 73s 236ms/step - loss: 3.0037 - lr: 0.0010\n","Epoch 109/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0010\n","Epoch 109: loss improved from 3.00367 to 3.00104, saving model to nextword1.h5\n","309/309 [==============================] - 71s 228ms/step - loss: 3.0010 - lr: 0.0010\n","Epoch 110/150\n","309/309 [==============================] - ETA: 0s - loss: 3.0023\n","Epoch 110: loss did not improve from 3.00104\n","309/309 [==============================] - 68s 221ms/step - loss: 3.0023 - lr: 0.0010\n","Epoch 111/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9983\n","Epoch 111: loss improved from 3.00104 to 2.99828, saving model to nextword1.h5\n","309/309 [==============================] - 72s 232ms/step - loss: 2.9983 - lr: 0.0010\n","Epoch 112/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9979\n","Epoch 112: loss improved from 2.99828 to 2.99793, saving model to nextword1.h5\n","309/309 [==============================] - 71s 228ms/step - loss: 2.9979 - lr: 0.0010\n","Epoch 113/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9981\n","Epoch 113: loss did not improve from 2.99793\n","309/309 [==============================] - 69s 223ms/step - loss: 2.9981 - lr: 0.0010\n","Epoch 114/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9941\n","Epoch 114: loss improved from 2.99793 to 2.99407, saving model to nextword1.h5\n","309/309 [==============================] - 72s 232ms/step - loss: 2.9941 - lr: 0.0010\n","Epoch 115/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9926\n","Epoch 115: loss improved from 2.99407 to 2.99256, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 2.9926 - lr: 0.0010\n","Epoch 116/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9946\n","Epoch 116: loss did not improve from 2.99256\n","309/309 [==============================] - 71s 230ms/step - loss: 2.9946 - lr: 0.0010\n","Epoch 117/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9909\n","Epoch 117: loss improved from 2.99256 to 2.99092, saving model to nextword1.h5\n","309/309 [==============================] - 70s 228ms/step - loss: 2.9909 - lr: 0.0010\n","Epoch 118/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9929\n","Epoch 118: loss did not improve from 2.99092\n","309/309 [==============================] - 70s 225ms/step - loss: 2.9929 - lr: 0.0010\n","Epoch 119/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9890\n","Epoch 119: loss improved from 2.99092 to 2.98904, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 2.9890 - lr: 0.0010\n","Epoch 120/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9875\n","Epoch 120: loss improved from 2.98904 to 2.98747, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 2.9875 - lr: 0.0010\n","Epoch 121/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9815\n","Epoch 121: loss improved from 2.98747 to 2.98148, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 2.9815 - lr: 0.0010\n","Epoch 122/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9817\n","Epoch 122: loss did not improve from 2.98148\n","309/309 [==============================] - 70s 227ms/step - loss: 2.9817 - lr: 0.0010\n","Epoch 123/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9820\n","Epoch 123: loss did not improve from 2.98148\n","309/309 [==============================] - 68s 222ms/step - loss: 2.9820 - lr: 0.0010\n","Epoch 124/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9775\n","Epoch 124: loss improved from 2.98148 to 2.97753, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 2.9775 - lr: 0.0010\n","Epoch 125/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9762\n","Epoch 125: loss improved from 2.97753 to 2.97621, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 2.9762 - lr: 0.0010\n","Epoch 126/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9791\n","Epoch 126: loss did not improve from 2.97621\n","309/309 [==============================] - 69s 224ms/step - loss: 2.9791 - lr: 0.0010\n","Epoch 127/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9852\n","Epoch 127: loss did not improve from 2.97621\n","309/309 [==============================] - 69s 222ms/step - loss: 2.9852 - lr: 0.0010\n","Epoch 128/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9726\n","Epoch 128: loss improved from 2.97621 to 2.97263, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 2.9726 - lr: 0.0010\n","Epoch 129/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9733\n","Epoch 129: loss did not improve from 2.97263\n","309/309 [==============================] - 70s 225ms/step - loss: 2.9733 - lr: 0.0010\n","Epoch 130/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9725\n","Epoch 130: loss improved from 2.97263 to 2.97248, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 2.9725 - lr: 0.0010\n","Epoch 131/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9718\n","Epoch 131: loss improved from 2.97248 to 2.97181, saving model to nextword1.h5\n","309/309 [==============================] - 73s 236ms/step - loss: 2.9718 - lr: 0.0010\n","Epoch 132/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9697\n","Epoch 132: loss improved from 2.97181 to 2.96971, saving model to nextword1.h5\n","309/309 [==============================] - 72s 233ms/step - loss: 2.9697 - lr: 0.0010\n","Epoch 133/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9695\n","Epoch 133: loss improved from 2.96971 to 2.96953, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 2.9695 - lr: 0.0010\n","Epoch 134/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9670\n","Epoch 134: loss improved from 2.96953 to 2.96698, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 2.9670 - lr: 0.0010\n","Epoch 135/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9649\n","Epoch 135: loss improved from 2.96698 to 2.96493, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 2.9649 - lr: 0.0010\n","Epoch 136/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9701\n","Epoch 136: loss did not improve from 2.96493\n","309/309 [==============================] - 71s 231ms/step - loss: 2.9701 - lr: 0.0010\n","Epoch 137/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9653\n","Epoch 137: loss did not improve from 2.96493\n","309/309 [==============================] - 68s 220ms/step - loss: 2.9653 - lr: 0.0010\n","Epoch 138/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9635\n","Epoch 138: loss improved from 2.96493 to 2.96351, saving model to nextword1.h5\n","309/309 [==============================] - 70s 227ms/step - loss: 2.9635 - lr: 0.0010\n","Epoch 139/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9623\n","Epoch 139: loss improved from 2.96351 to 2.96229, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 2.9623 - lr: 0.0010\n","Epoch 140/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9609\n","Epoch 140: loss improved from 2.96229 to 2.96087, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 2.9609 - lr: 0.0010\n","Epoch 141/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9599\n","Epoch 141: loss improved from 2.96087 to 2.95993, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 2.9599 - lr: 0.0010\n","Epoch 142/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9610\n","Epoch 142: loss did not improve from 2.95993\n","309/309 [==============================] - 71s 229ms/step - loss: 2.9610 - lr: 0.0010\n","Epoch 143/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9598\n","Epoch 143: loss improved from 2.95993 to 2.95975, saving model to nextword1.h5\n","309/309 [==============================] - 71s 229ms/step - loss: 2.9598 - lr: 0.0010\n","Epoch 144/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9621\n","Epoch 144: loss did not improve from 2.95975\n","309/309 [==============================] - 69s 224ms/step - loss: 2.9621 - lr: 0.0010\n","Epoch 145/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9553\n","Epoch 145: loss improved from 2.95975 to 2.95527, saving model to nextword1.h5\n","309/309 [==============================] - 72s 234ms/step - loss: 2.9553 - lr: 0.0010\n","Epoch 146/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9557\n","Epoch 146: loss did not improve from 2.95527\n","309/309 [==============================] - 69s 223ms/step - loss: 2.9557 - lr: 0.0010\n","Epoch 147/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9538\n","Epoch 147: loss improved from 2.95527 to 2.95381, saving model to nextword1.h5\n","309/309 [==============================] - 71s 230ms/step - loss: 2.9538 - lr: 0.0010\n","Epoch 148/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9542\n","Epoch 148: loss did not improve from 2.95381\n","309/309 [==============================] - 71s 229ms/step - loss: 2.9542 - lr: 0.0010\n","Epoch 149/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9539\n","Epoch 149: loss did not improve from 2.95381\n","309/309 [==============================] - 68s 222ms/step - loss: 2.9539 - lr: 0.0010\n","Epoch 150/150\n","309/309 [==============================] - ETA: 0s - loss: 2.9529\n","Epoch 150: loss improved from 2.95381 to 2.95294, saving model to nextword1.h5\n","309/309 [==============================] - 73s 235ms/step - loss: 2.9529 - lr: 0.0010\n"]},{"output_type":"execute_result","data":{"text/plain":["<keras.callbacks.History at 0x7f4d71cd8640>"]},"metadata":{},"execution_count":69}]},{"cell_type":"markdown","source":["#### Evaluating the model"],"metadata":{"id":"MQsEn-ColVil"}},{"cell_type":"code","source":["# def next_word_pred(model, tokenizer, text):\n","#   encoded_txt = tokenizer.texts_to_sequences([text])[0]\n","#   encoded_txt = np.array(encoded_txt)\n","\n","#   next_word_token_prob = model.predict(encoded_txt)\n","#   next_word_toekn_max_prob = np.argmax(next_word_token_prob)\n","\n","#   for word, token in tokenizer.word_index.items():\n","#     if token == next_word_toekn_max_prob:\n","#         return word\n","  \n","#   return None\n"],"metadata":{"id":"Rlc7L6y7n4U2","executionInfo":{"status":"ok","timestamp":1674762956631,"user_tz":300,"elapsed":130,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["def next_word_pred(model, tokenizer, word_token):\n","  next_word_token_prob = model.predict(word_token)\n","  next_word_toekn_max_prob = np.argmax(next_word_token_prob)\n","\n","  for word, token in tokenizer.word_index.items():\n","    if token == next_word_toekn_max_prob:\n","        return word\n","  \n","  return None"],"metadata":{"id":"iErS76zmqT1j","executionInfo":{"status":"ok","timestamp":1674763161385,"user_tz":300,"elapsed":156,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}}},"execution_count":118,"outputs":[]},{"cell_type":"code","source":["model.predict(51)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":320},"id":"85DzxXbKrIg6","executionInfo":{"status":"error","timestamp":1674763392552,"user_tz":300,"elapsed":198,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"183ecccb-a878-441e-f5f1-65e3760a7c09"},"execution_count":125,"outputs":[{"output_type":"error","ename":"IndexError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-125-f4476059afb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m51\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/tensorflow/python/framework/tensor_shape.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    907\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    908\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_v2_behavior\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 909\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    910\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m           \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdims\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mIndexError\u001b[0m: tuple index out of range"]}]},{"cell_type":"code","source":["y_pred = []\n","for word_token in x:\n","  print(word_token)\n","  next_word_token_prob, unk = model.predict([word_token])\n","  next_word_toekn_max_prob = np.argmax(next_word_token_prob)\n","  print(next_word_toekn_max_prob)\n","  y_pred.append(next_word_toekn_max_prob)\n","\n","y_pred"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":410},"id":"g6Kcgru17m-6","executionInfo":{"status":"error","timestamp":1674763465731,"user_tz":300,"elapsed":171,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"34e1325b-2ee1-4e68-94bf-cbc349619b58"},"execution_count":127,"outputs":[{"output_type":"stream","name":"stdout","text":["51\n"]},{"output_type":"error","ename":"ValueError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","\u001b[0;32m<ipython-input-127-8f5f960bcfb9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword_token\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword_token\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m   \u001b[0mnext_word_token_prob\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0munk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword_token\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m   \u001b[0mnext_word_toekn_max_prob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_token_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m   \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_word_toekn_max_prob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m       \u001b[0;32mraise\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwith_traceback\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiltered_tb\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m       \u001b[0;32mdel\u001b[0m \u001b[0mfiltered_tb\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mselect_data_adapter\u001b[0;34m(x, y)\u001b[0m\n\u001b[1;32m    983\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0madapter_cls\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    984\u001b[0m     \u001b[0;31m# TODO(scottzhu): This should be a less implementation-specific error.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 985\u001b[0;31m     raise ValueError(\n\u001b[0m\u001b[1;32m    986\u001b[0m         \u001b[0;34m\"Failed to find data adapter that can handle \"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    987\u001b[0m         \"input: {}, {}\".format(\n","\u001b[0;31mValueError\u001b[0m: Failed to find data adapter that can handle input: (<class 'list'> containing values of types {\"<class 'numpy.int64'>\"}), <class 'NoneType'>"]}]},{"cell_type":"code","source":["text = \"dull\"\n","next_word_pred(model, tokenizer, text)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":54},"id":"CVxM_g-8l55k","executionInfo":{"status":"ok","timestamp":1674762963803,"user_tz":300,"elapsed":153,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"67c8c5c3-07c8-4df9-fcba-6859b9dca527"},"execution_count":116,"outputs":[{"output_type":"stream","name":"stdout","text":["1/1 [==============================] - 0s 22ms/step\n"]},{"output_type":"execute_result","data":{"text/plain":["'pain'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":116}]},{"cell_type":"code","source":["tokenizer.word_index"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8Sn3I_Qql_ta","executionInfo":{"status":"ok","timestamp":1674762219936,"user_tz":300,"elapsed":152,"user":{"displayName":"Vaishanth Ramaraj","userId":"11469613983025473676"}},"outputId":"b3c1a660-623e-417a-aabf-1790edc1d018"},"execution_count":84,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'the': 1,\n"," 'to': 2,\n"," 'and': 3,\n"," 'he': 4,\n"," 'his': 5,\n"," 'of': 6,\n"," 'was': 7,\n"," 'it': 8,\n"," 'in': 9,\n"," 'had': 10,\n"," 'that': 11,\n"," 'a': 12,\n"," 'as': 13,\n"," 'gregor': 14,\n"," 'she': 15,\n"," 'with': 16,\n"," 'him': 17,\n"," 'her': 18,\n"," 'not': 19,\n"," 'but': 20,\n"," 'would': 21,\n"," 'at': 22,\n"," 'for': 23,\n"," 'they': 24,\n"," 'on': 25,\n"," 'all': 26,\n"," 'room': 27,\n"," 'could': 28,\n"," 'be': 29,\n"," 'out': 30,\n"," 'from': 31,\n"," 'have': 32,\n"," 'if': 33,\n"," 'so': 34,\n"," 'there': 35,\n"," 'father': 36,\n"," 'been': 37,\n"," 'now': 38,\n"," 'gregors': 39,\n"," 'then': 40,\n"," 'sister': 41,\n"," 'this': 42,\n"," 'mother': 43,\n"," 'into': 44,\n"," 'back': 45,\n"," 'up': 46,\n"," 'door': 47,\n"," 'even': 48,\n"," 'did': 49,\n"," 'no': 50,\n"," 'one': 51,\n"," 'what': 52,\n"," 'himself': 53,\n"," 'were': 54,\n"," 'when': 55,\n"," 'more': 56,\n"," 'about': 57,\n"," 'them': 58,\n"," 'their': 59,\n"," 'way': 60,\n"," 'i': 61,\n"," 'time': 62,\n"," 'only': 63,\n"," 'said': 64,\n"," 'by': 65,\n"," 'than': 66,\n"," 'you': 67,\n"," 'little': 68,\n"," 'get': 69,\n"," 'just': 70,\n"," 'other': 71,\n"," 'first': 72,\n"," 'any': 73,\n"," 'or': 74,\n"," 'much': 75,\n"," 'go': 76,\n"," 'do': 77,\n"," 'still': 78,\n"," 'head': 79,\n"," 'made': 80,\n"," 'where': 81,\n"," 'some': 82,\n"," 'samsa': 83,\n"," 'like': 84,\n"," 'well': 85,\n"," 'see': 86,\n"," 'again': 87,\n"," 'open': 88,\n"," 'chief': 89,\n"," 'who': 90,\n"," 'over': 91,\n"," 'after': 92,\n"," 'its': 93,\n"," 'though': 94,\n"," 'while': 95,\n"," 'thought': 96,\n"," 'which': 97,\n"," 'soon': 98,\n"," 'went': 99,\n"," 'before': 100,\n"," 'clerk': 101,\n"," 'left': 102,\n"," 'come': 103,\n"," 'family': 104,\n"," 'bed': 105,\n"," 'too': 106,\n"," 'down': 107,\n"," 'away': 108,\n"," 'came': 109,\n"," 'an': 110,\n"," 'very': 111,\n"," 'without': 112,\n"," 'two': 113,\n"," 'let': 114,\n"," 'day': 115,\n"," 'looked': 116,\n"," 'being': 117,\n"," 'wanted': 118,\n"," 'is': 119,\n"," 'gentlemen': 120,\n"," 'everything': 121,\n"," 'against': 122,\n"," 'things': 123,\n"," 'work': 124,\n"," 'we': 125,\n"," 'seemed': 126,\n"," 'look': 127,\n"," 'how': 128,\n"," 'something': 129,\n"," 'parents': 130,\n"," 'mr': 131,\n"," 'three': 132,\n"," 'grete': 133,\n"," 'got': 134,\n"," 'round': 135,\n"," 'side': 136,\n"," 'body': 137,\n"," 'quite': 138,\n"," 'floor': 139,\n"," 'really': 140,\n"," 'legs': 141,\n"," 'although': 142,\n"," 'because': 143,\n"," 'make': 144,\n"," 'long': 145,\n"," 'morning': 146,\n"," 'able': 147,\n"," 'window': 148,\n"," 'onto': 149,\n"," 'began': 150,\n"," 'my': 151,\n"," 'through': 152,\n"," 'already': 153,\n"," 'under': 154,\n"," 'flat': 155,\n"," 'off': 156,\n"," 'heard': 157,\n"," 'longer': 158,\n"," 'right': 159,\n"," 'slowly': 160,\n"," 'better': 161,\n"," 'next': 162,\n"," 'voice': 163,\n"," 'took': 164,\n"," 'around': 165,\n"," 'here': 166,\n"," 'evening': 167,\n"," 'hand': 168,\n"," 'table': 169,\n"," 'whole': 170,\n"," 'used': 171,\n"," 'always': 172,\n"," 'eyes': 173,\n"," 'never': 174,\n"," 'home': 175,\n"," 'know': 176,\n"," 'once': 177,\n"," 'perhaps': 178,\n"," 'herself': 179,\n"," 'behind': 180,\n"," 'lay': 181,\n"," 'me': 182,\n"," 'feel': 183,\n"," 'must': 184,\n"," 'often': 185,\n"," 'move': 186,\n"," 'enough': 187,\n"," 'say': 188,\n"," 'taken': 189,\n"," 'hardly': 190,\n"," 'happened': 191,\n"," 'your': 192,\n"," 'food': 193,\n"," 'think': 194,\n"," 'called': 195,\n"," 'each': 196,\n"," 'almost': 197,\n"," 'course': 198,\n"," 'opened': 199,\n"," 'also': 200,\n"," 'asked': 201,\n"," 'chair': 202,\n"," 'women': 203,\n"," 'sat': 204,\n"," 'these': 205,\n"," 'chest': 206,\n"," 'furniture': 207,\n"," 'put': 208,\n"," 'nothing': 209,\n"," 'arms': 210,\n"," 'hands': 211,\n"," 'seen': 212,\n"," 'might': 213,\n"," 'became': 214,\n"," 'same': 215,\n"," 'held': 216,\n"," 'good': 217,\n"," 'become': 218,\n"," 'attention': 219,\n"," 'living': 220,\n"," 'couch': 221,\n"," 'turned': 222,\n"," 'sleep': 223,\n"," 'doing': 224,\n"," 'business': 225,\n"," 'can': 226,\n"," 'felt': 227,\n"," 'close': 228,\n"," 'later': 229,\n"," 'should': 230,\n"," 'words': 231,\n"," 'im': 232,\n"," 'anything': 233,\n"," 'dont': 234,\n"," 'possible': 235,\n"," 'help': 236,\n"," 'immediately': 237,\n"," 'face': 238,\n"," 'straight': 239,\n"," 'take': 240,\n"," 'pressed': 241,\n"," 'hard': 242,\n"," 'pushed': 243,\n"," 'didnt': 244,\n"," 'life': 245,\n"," 'ill': 246,\n"," 'probably': 247,\n"," 'forward': 248,\n"," 'least': 249,\n"," 'leave': 250,\n"," 'us': 251,\n"," 'reason': 252,\n"," 'ran': 253,\n"," 'front': 254,\n"," 'old': 255,\n"," 'tried': 256,\n"," 'times': 257,\n"," 'saw': 258,\n"," 'quickly': 259,\n"," 'certainly': 260,\n"," 'want': 261,\n"," 'making': 262,\n"," 'night': 263,\n"," 'keep': 264,\n"," 'moved': 265,\n"," 'hear': 266,\n"," 'every': 267,\n"," 'mrs': 268,\n"," 'done': 269,\n"," 'key': 270,\n"," 'stood': 271,\n"," 'playing': 272,\n"," 'new': 273,\n"," 'violin': 274,\n"," 'whats': 275,\n"," 'however': 276,\n"," 'effort': 277,\n"," 'told': 278,\n"," 'desk': 279,\n"," 'especially': 280,\n"," 'thats': 281,\n"," 'moving': 282,\n"," 'set': 283,\n"," 'give': 284,\n"," 'doors': 285,\n"," 'most': 286,\n"," 'themselves': 287,\n"," 'others': 288,\n"," 'cant': 289,\n"," 'despite': 290,\n"," 'last': 291,\n"," 'everyone': 292,\n"," 'clear': 293,\n"," 'money': 294,\n"," 'since': 295,\n"," 'tears': 296,\n"," 'great': 297,\n"," 'turn': 298,\n"," 'until': 299,\n"," 'eat': 300,\n"," 'middle': 301,\n"," 'moment': 302,\n"," 'upright': 303,\n"," 'covered': 304,\n"," 'stopped': 305,\n"," 'pain': 306,\n"," 'own': 307,\n"," 'together': 308,\n"," 'another': 309,\n"," 'train': 310,\n"," 'half': 311,\n"," 'yes': 312,\n"," 'having': 313,\n"," 'ever': 314,\n"," 'carefully': 315,\n"," 'dressed': 316,\n"," 'fell': 317,\n"," 'finally': 318,\n"," 'stay': 319,\n"," 'clearly': 320,\n"," 'will': 321,\n"," 'use': 322,\n"," 'far': 323,\n"," 'remained': 324,\n"," 'needed': 325,\n"," 'knew': 326,\n"," 'why': 327,\n"," 'please': 328,\n"," 'hes': 329,\n"," 'kitchen': 330,\n"," 'job': 331,\n"," 'condition': 332,\n"," 'realised': 333,\n"," 'mouth': 334,\n"," 'leant': 335,\n"," 'several': 336,\n"," 'looking': 337,\n"," 'doorway': 338,\n"," 'found': 339,\n"," 'charwoman': 340,\n"," 'rest': 341,\n"," 'nice': 342,\n"," 'position': 343,\n"," 'shut': 344,\n"," 'place': 345,\n"," 'maybe': 346,\n"," 'thing': 347,\n"," 'notice': 348,\n"," 'boss': 349,\n"," 'fall': 350,\n"," 'years': 351,\n"," 'seven': 352,\n"," 'oclock': 353,\n"," 'noise': 354,\n"," 'man': 355,\n"," 'completely': 356,\n"," 'near': 357,\n"," 'need': 358,\n"," 'bring': 359,\n"," 'force': 360,\n"," 'turning': 361,\n"," 'order': 362,\n"," 'calm': 363,\n"," 'loud': 364,\n"," 'maid': 365,\n"," 'quiet': 366,\n"," 'whether': 367,\n"," 'else': 368,\n"," 'simply': 369,\n"," 'easy': 370,\n"," 'drawers': 371,\n"," 'understand': 372,\n"," 'kind': 373,\n"," 'sign': 374,\n"," 'breath': 375,\n"," 'taking': 376,\n"," 'reached': 377,\n"," 'end': 378,\n"," 'fully': 379,\n"," 'dish': 380,\n"," 'few': 381,\n"," 'hissister': 382,\n"," 'stand': 383,\n"," 'tired': 384,\n"," 'cleaner': 385,\n"," 'lifted': 386,\n"," 'walls': 387,\n"," 'spread': 388,\n"," 'travelling': 389,\n"," 'picture': 390,\n"," 'forget': 391,\n"," 'unable': 392,\n"," 'sleeping': 393,\n"," 'oh': 394,\n"," 'getting': 395,\n"," 'during': 396,\n"," 'are': 397,\n"," 'try': 398,\n"," 'given': 399,\n"," 'five': 400,\n"," 'struck': 401,\n"," 'word': 402,\n"," 'peace': 403,\n"," 'caused': 404,\n"," 'painfully': 405,\n"," 'present': 406,\n"," 'state': 407,\n"," 'kept': 408,\n"," 'carpet': 409,\n"," 'someone': 410,\n"," 'question': 411,\n"," 'anyway': 412,\n"," 'nearly': 413,\n"," 'youre': 414,\n"," 'year': 415,\n"," 'strength': 416,\n"," 'sight': 417,\n"," 'gave': 418,\n"," 'hall': 419,\n"," 'hold': 420,\n"," 'lock': 421,\n"," 'ofthe': 422,\n"," 'uniform': 423,\n"," 'chance': 424,\n"," 'going': 425,\n"," 'towards': 426,\n"," 'rushed': 427,\n"," 'lost': 428,\n"," 'forgotten': 429,\n"," 'running': 430,\n"," 'crawling': 431,\n"," 'closed': 432,\n"," 'noticed': 433,\n"," 'actually': 434,\n"," 'days': 435,\n"," 'music': 436,\n"," 'totally': 437,\n"," 'bent': 438,\n"," 'slightly': 439,\n"," 'human': 440,\n"," 'lower': 441,\n"," 'couldnt': 442,\n"," 'god': 443,\n"," 'anyone': 444,\n"," 'friendly': 445,\n"," 'early': 446,\n"," 'whenever': 447,\n"," 'eating': 448,\n"," 'id': 449,\n"," 'tell': 450,\n"," 'alarm': 451,\n"," 'past': 452,\n"," 'quarter': 453,\n"," 'rush': 454,\n"," 'doctor': 455,\n"," 'wrong': 456,\n"," 'leaving': 457,\n"," 'sides': 458,\n"," 'breakfast': 459,\n"," 'instead': 460,\n"," 'serious': 461,\n"," 'part': 462,\n"," 'learned': 463,\n"," 'air': 464,\n"," 'itwas': 465,\n"," 'free': 466,\n"," 'street': 467,\n"," 'expected': 468,\n"," 'patient': 469,\n"," 'locked': 470,\n"," 'across': 471,\n"," 'necessary': 472,\n"," 'show': 473,\n"," 'speak': 474,\n"," 'conversation': 475,\n"," 'carry': 476,\n"," 'late': 477,\n"," 'beside': 478,\n"," 'inthe': 479,\n"," 'suddenly': 480,\n"," 'wait': 481,\n"," 'listen': 482,\n"," 'loudly': 483,\n"," 'listening': 484,\n"," 'using': 485,\n"," 'start': 486,\n"," 'difficult': 487,\n"," 'along': 488,\n"," 'meal': 489,\n"," 'hours': 490,\n"," 'wall': 491,\n"," 'landing': 492,\n"," 'enormous': 493,\n"," 'foot': 494,\n"," 'both': 495,\n"," 'urge': 496,\n"," 'flew': 497,\n"," 'allowed': 498,\n"," 'fathers': 499,\n"," 'finished': 500,\n"," 'slammed': 501,\n"," 'white': 502,\n"," 'sometimes': 503,\n"," 'either': 504,\n"," 'sisters': 505,\n"," 'full': 506,\n"," 'surprise': 507,\n"," 'carried': 508,\n"," 'earlier': 509,\n"," 'thedoor': 510,\n"," 'slow': 511,\n"," 'crawl': 512,\n"," 'ceiling': 513,\n"," 'mothers': 514,\n"," 'meant': 515,\n"," 'immobile': 516,\n"," 'shouted': 517,\n"," 'sit': 518,\n"," 'appeared': 519,\n"," 'gentleman': 520,\n"," 'couldsee': 521,\n"," 'brown': 522,\n"," 'wasnt': 523,\n"," 'small': 524,\n"," 'four': 525,\n"," 'hung': 526,\n"," 'cut': 527,\n"," 'lady': 528,\n"," 'fur': 529,\n"," 'raising': 530,\n"," 'theres': 531,\n"," 'drew': 532,\n"," 'touched': 533,\n"," 'overcome': 534,\n"," 'live': 535,\n"," 'best': 536,\n"," 'sort': 537,\n"," 'hope': 538,\n"," 'pay': 539,\n"," 'clock': 540,\n"," 'quietly': 541,\n"," 'forwards': 542,\n"," 'fresh': 543,\n"," 'office': 544,\n"," 'entirely': 545,\n"," 'usual': 546,\n"," 'hurriedly': 547,\n"," 'short': 548,\n"," 'aware': 549,\n"," 'gently': 550,\n"," 'answered': 551,\n"," 'remove': 552,\n"," 'between': 553,\n"," 'habit': 554,\n"," 'thoughts': 555,\n"," 'lying': 556,\n"," 'today': 557,\n"," 'throw': 558,\n"," 'push': 559,\n"," 'those': 560,\n"," 'different': 561,\n"," 'itself': 562,\n"," 'direction': 563,\n"," 'hit': 564,\n"," 'pushing': 565,\n"," 'harder': 566,\n"," 'view': 567,\n"," 'confidence': 568,\n"," 'entire': 569,\n"," 'raised': 570,\n"," 'happen': 571,\n"," 'concern': 572,\n"," 'strong': 573,\n"," 'people': 574,\n"," 'find': 575,\n"," 'call': 576,\n"," 'employees': 577,\n"," 'sound': 578,\n"," 'annoyed': 579,\n"," 'fallen': 580,\n"," 'idea': 581,\n"," 'working': 582,\n"," 'glad': 583,\n"," 'sure': 584,\n"," 'seriously': 585,\n"," 'theywere': 586,\n"," 'speaking': 587,\n"," 'astonished': 588,\n"," 'seem': 589,\n"," 'alright': 590,\n"," 'knowing': 591,\n"," 'animal': 592,\n"," 'entrance': 593,\n"," 'locksmith': 594,\n"," 'threw': 595,\n"," 'unfortunately': 596,\n"," 'turnedround': 597,\n"," 'holding': 598,\n"," 'reading': 599,\n"," 'exactly': 600,\n"," 'earn': 601,\n"," 'nobody': 602,\n"," 'impossible': 603,\n"," 'started': 604,\n"," 'save': 605,\n"," 'talk': 606,\n"," 'pleasure': 607,\n"," 'spare': 608,\n"," 'stick': 609,\n"," 'merely': 610,\n"," 'intentions': 611,\n"," 'tip': 612,\n"," 'obviously': 613,\n"," 'hurried': 614,\n"," 'light': 615,\n"," 'filled': 616,\n"," 'milk': 617,\n"," 'hehad': 618,\n"," 'crawled': 619,\n"," 'hadbeen': 620,\n"," 'empty': 621,\n"," 'nonetheless': 622,\n"," 'forced': 623,\n"," 'anxiously': 624,\n"," 'outside': 625,\n"," 'rather': 626,\n"," 'feet': 627,\n"," 'brought': 628,\n"," 'gone': 629,\n"," 'month': 630,\n"," 'startled': 631,\n"," 'asleep': 632,\n"," 'breathe': 633,\n"," 'send': 634,\n"," 'dinner': 635,\n"," 'fetch': 636,\n"," 'decided': 637,\n"," 'silent': 638,\n"," 'coming': 639,\n"," 'sheet': 640,\n"," 'afterall': 641,\n"," 'pulled': 642,\n"," 'indeed': 643,\n"," 'writing': 644,\n"," 'high': 645,\n"," 'ground': 646,\n"," 'coat': 647,\n"," 'pockets': 648,\n"," 'broom': 649,\n"," 'briefly': 650,\n"," 'rid': 651,\n"," 'horrible': 652,\n"," 'hat': 653,\n"," 'heavy': 654,\n"," 'rain': 655,\n"," 'bit': 656,\n"," 'todo': 657,\n"," 'rolled': 658,\n"," 'whenhe': 659,\n"," 'hell': 660,\n"," 'thetime': 661,\n"," 'salesmen': 662,\n"," 'ought': 663,\n"," 'tothink': 664,\n"," 'ago': 665,\n"," 'talking': 666,\n"," 'hearing': 667,\n"," 'ive': 668,\n"," 'debt': 669,\n"," 'big': 670,\n"," 'change': 671,\n"," 'hecould': 672,\n"," 'lively': 673,\n"," 'avoid': 674,\n"," 'report': 675,\n"," 'understanding': 676,\n"," 'yet': 677,\n"," 'apart': 678,\n"," 'somebody': 679,\n"," 'deep': 680,\n"," 'mixed': 681,\n"," 'properly': 682,\n"," 'saying': 683,\n"," 'explanation': 684,\n"," 'ready': 685,\n"," 'slight': 686,\n"," 'simple': 687,\n"," 'directions': 688,\n"," 'trying': 689,\n"," 'weight': 690,\n"," 'ofhis': 691,\n"," 'followed': 692,\n"," 'occurred': 693,\n"," 'afraid': 694,\n"," 'whatever': 695,\n"," 'consideration': 696,\n"," 'narrow': 697,\n"," 'ask': 698,\n"," 'outof': 699,\n"," 'mind': 700,\n"," 'fact': 701,\n"," 'difficulty': 702,\n"," 'caught': 703,\n"," 'firm': 704,\n"," 'thechief': 705,\n"," 'upset': 706,\n"," 'decision': 707,\n"," 'clerks': 708,\n"," 'chiefclerk': 709,\n"," 'has': 710,\n"," 'isnt': 711,\n"," 'evenings': 712,\n"," 'town': 713,\n"," 'youll': 714,\n"," 'silence': 715,\n"," 'crying': 716,\n"," 'worry': 717,\n"," 'suitable': 718,\n"," 'happening': 719,\n"," 'behaviour': 720,\n"," 'answer': 721,\n"," 'causing': 722,\n"," 'employer': 723,\n"," 'am': 724,\n"," 'giving': 725,\n"," 'nor': 726,\n"," 'waste': 727,\n"," 'learn': 728,\n"," 'sir': 729,\n"," 'suffer': 730,\n"," 'shocked': 731,\n"," 'hisparents': 732,\n"," 'contrast': 733,\n"," 'skirts': 734,\n"," 'drawn': 735,\n"," 'tomake': 736,\n"," 'meanwhile': 737,\n"," 'lack': 738,\n"," 'flowed': 739,\n"," 'onthe': 740,\n"," 'double': 741,\n"," 'occupied': 742,\n"," 'hair': 743,\n"," 'steps': 744,\n"," 'disappeared': 745,\n"," 'above': 746,\n"," 'large': 747,\n"," 'smile': 748,\n"," 'usually': 749,\n"," 'partly': 750,\n"," 'stretched': 751,\n"," 'mood': 752,\n"," 'provide': 753,\n"," 'future': 754,\n"," 'persuade': 755,\n"," 'understood': 756,\n"," 'landed': 757,\n"," 'infront': 758,\n"," 'outstretched': 759,\n"," 'sake': 760,\n"," 'fled': 761,\n"," 'relatively': 762,\n"," 'newspaper': 763,\n"," 'drive': 764,\n"," 'appeals': 765,\n"," 'further': 766,\n"," 'shove': 767,\n"," 'dark': 768,\n"," 'feeling': 769,\n"," 'smell': 770,\n"," 'hewas': 771,\n"," 'darkness': 772,\n"," 'waited': 773,\n"," 'toe': 774,\n"," 'remain': 775,\n"," 'uneasy': 776,\n"," 'regret': 777,\n"," 'spent': 778,\n"," 'hunger': 779,\n"," 'bear': 780,\n"," 'test': 781,\n"," 'control': 782,\n"," 'realise': 783,\n"," 'thecouch': 784,\n"," 'bare': 785,\n"," 'cheese': 786,\n"," 'comfortable': 787,\n"," 'liked': 788,\n"," 'finger': 789,\n"," 'self': 790,\n"," 'themorning': 791,\n"," 'eaten': 792,\n"," 'itwould': 793,\n"," 'news': 794,\n"," 'ate': 795,\n"," 'earned': 796,\n"," 'conservatory': 797,\n"," 'christmas': 798,\n"," 'pull': 799,\n"," 'repeated': 800,\n"," 'lot': 801,\n"," 'closer': 802,\n"," 'sofa': 803,\n"," 'child': 804,\n"," 'clothes': 805,\n"," 'experienced': 806,\n"," 'easier': 807,\n"," 'didnot': 808,\n"," 'letting': 809,\n"," 'hismother': 810,\n"," 'careful': 811,\n"," 'admit': 812,\n"," 'worn': 813,\n"," 'arm': 814,\n"," 'lets': 815,\n"," 'chase': 816,\n"," 'gretes': 817,\n"," 'stepped': 818,\n"," 'stayed': 819,\n"," 'shaking': 820,\n"," 'broke': 821,\n"," 'mean': 822,\n"," 'hisway': 823,\n"," 'stop': 824,\n"," 'gold': 825,\n"," 'buttons': 826,\n"," 'cap': 827,\n"," 'apple': 828,\n"," 'contrary': 829,\n"," 'absolutely': 830,\n"," 'clean': 831,\n"," 'livingroom': 832,\n"," 'chairs': 833,\n"," 'corner': 834,\n"," 'play': 835,\n"," 'beards': 836,\n"," 'thekitchen': 837,\n"," 'played': 838,\n"," 'young': 839,\n"," 'withthe': 840,\n"," 'neck': 841,\n"," 'our': 842,\n"," 'dead': 843,\n"," 'daughter': 844,\n"," 'letters': 845,\n"," 'woke': 846,\n"," 'dreams': 847,\n"," 'onhis': 848,\n"," 'belly': 849,\n"," 'many': 850,\n"," 'thin': 851,\n"," 'size': 852,\n"," 'dream': 853,\n"," 'proper': 854,\n"," 'familiar': 855,\n"," 'collection': 856,\n"," 'salesman': 857,\n"," 'recently': 858,\n"," 'frame': 859,\n"," 'fitted': 860,\n"," 'dull': 861,\n"," 'weather': 862,\n"," 'drops': 863,\n"," 'hitting': 864,\n"," 'pane': 865,\n"," 'sad': 866,\n"," 'allthis': 867,\n"," 'presentstate': 868,\n"," 'curse': 869,\n"," 'worries': 870,\n"," 'bad': 871,\n"," 'itch': 872,\n"," 'lift': 873,\n"," 'cold': 874,\n"," 'slid': 875,\n"," 'makes': 876,\n"," 'stupid': 877,\n"," 'youve': 878,\n"," 'forinstance': 879,\n"," 'house': 880,\n"," 'breakfasts': 881,\n"," 'spot': 882,\n"," 'thatwould': 883,\n"," 'hed': 884,\n"," 'sitting': 885,\n"," 'themoney': 886,\n"," 'definitely': 887,\n"," 'ofdrawers': 888,\n"," 'six': 889,\n"," 'rung': 890,\n"," 'true': 891,\n"," 'slept': 892,\n"," 'peacefully': 893,\n"," 'catch': 894,\n"," 'mad': 895,\n"," 'andthe': 896,\n"," 'particularly': 897,\n"," 'bosss': 898,\n"," 'anger': 899,\n"," 'hewould': 900,\n"," 'fifteen': 901,\n"," 'wouldcertainly': 902,\n"," 'accuse': 903,\n"," 'son': 904,\n"," 'hungrier': 905,\n"," 'cautious': 906,\n"," 'knock': 907,\n"," 'somewhere': 908,\n"," 'painful': 909,\n"," 'explain': 910,\n"," 'circumstances': 911,\n"," 'thank': 912,\n"," 'members': 913,\n"," 'fist': 914,\n"," 'arent': 915,\n"," 'individual': 916,\n"," 'whispered': 917,\n"," 'opening': 918,\n"," 'consider': 919,\n"," 'sensible': 920,\n"," 'conclusions': 921,\n"," 'remembered': 922,\n"," 'inbed': 923,\n"," 'matter': 924,\n"," 'covers': 925,\n"," 'broad': 926,\n"," 'moreover': 927,\n"," 'bend': 928,\n"," 'stretch': 929,\n"," 'managed': 930,\n"," 'bodyout': 931,\n"," 'sensitive': 932,\n"," 'top': 933,\n"," 'easily': 934,\n"," 'eventually': 935,\n"," 'thefresh': 936,\n"," 'miracle': 937,\n"," 'injured': 938,\n"," 'carryon': 939,\n"," 'price': 940,\n"," 'sacrifice': 941,\n"," 'direct': 942,\n"," 'fog': 943,\n"," 'breathing': 944,\n"," 'backto': 945,\n"," 'real': 946,\n"," 'task': 947,\n"," 'falling': 948,\n"," 'raise': 949,\n"," 'forth': 950,\n"," 'swang': 951,\n"," 'ten': 952,\n"," 'froze': 953,\n"," 'asthey': 954,\n"," 'theyre': 955,\n"," 'company': 956,\n"," 'slightest': 957,\n"," 'hecouldnt': 958,\n"," 'spend': 959,\n"," 'couple': 960,\n"," 'enquiries': 961,\n"," 'innocent': 962,\n"," 'hishead': 963,\n"," 'rubbed': 964,\n"," 'polished': 965,\n"," 'boots': 966,\n"," 'lethim': 967,\n"," 'saidgregor': 968,\n"," 'wants': 969,\n"," 'week': 970,\n"," 'paper': 971,\n"," 'fretsaw': 972,\n"," 'hanging': 973,\n"," 'ourselves': 974,\n"," 'unwell': 975,\n"," 'fortunately': 976,\n"," 'knocking': 977,\n"," 'cry': 978,\n"," 'danger': 979,\n"," 'thesame': 980,\n"," 'likethat': 981,\n"," 'abandoning': 982,\n"," 'wasonly': 983,\n"," 'excuse': 984,\n"," 'sacked': 985,\n"," 'athim': 986,\n"," 'worried': 987,\n"," 'yourself': 988,\n"," 'fail': 989,\n"," 'ina': 990,\n"," 'unheard': 991,\n"," 'behalf': 992,\n"," 'person': 993,\n"," 'showing': 994,\n"," 'failure': 995,\n"," 'stubbornness': 996,\n"," 'intended': 997,\n"," 'cause': 998,\n"," 'grant': 999,\n"," 'forgetting': 1000,\n"," ...}"]},"metadata":{},"execution_count":84}]},{"cell_type":"code","source":[],"metadata":{"id":"uo_7ft8Xm2_v"},"execution_count":null,"outputs":[]}]}